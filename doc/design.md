## 1. RL算法选择
选择PPO原因：
- 适合在线学习场景
- 有KL约束防止策略崩溃
- 实验显示比DQN更适合文本生成任务

## 2. 动作空间设计
分层处理方案：
1. LLM生成5个候选动作
2. RL策略网络评分选择最优
3. 动态过滤无效语法动作

## 3. 状态表示
包含三部分：
- 当前房间描述（Jericho原始输出）
- 物品清单（JSON格式）
- 最近3轮历史（压缩摘要）
